{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising MCC Exploration\n",
    "\n",
    "This notebook logs exploratory results on adding teleportation on MCC with state coverage visualisation. Used for rough initial exploration.\n",
    "\n",
    "21/01/2024\n",
    "- Naive teleportation to argmax works\n",
    "- Longer episodes are better than shorter\n",
    "- Different intrinsic rewards show significantly different behavior\n",
    "- Even naively, general improvement over pure intrinsic\n",
    "- Fails to beat intrinsic + extrinsic: perhaps this is due to negative extrinsic reward revealing data on target? Not comparable, and I think fully explored in that reward shifting paper\n",
    "- Keeps teleporting to same target\n",
    "- This may be a problem with DDPG\n",
    "\n",
    "\n",
    "28/01/2024\n",
    "- Probabilistic teleportation work well\n",
    "- Environment reset stochasticity is important\n",
    "- Time limit aware Q functions are difficult to train!\n",
    "- Proposal: Dynamic Truncation!\n",
    "\n",
    "4/02/2024\n",
    "- ICM and RND leads to inherently different results - RND should be prioritised\n",
    "- CATS fails to improve over baseline on RND with fixed reset, but does in ICM. After reset, the new trajectory follows the previous trajectory too closely, while resetting from the start leads to more divergence across the entire episode (and hence more exploration)\n",
    "- Fixing the reset states leads to improved analysis\n",
    "- Policy function gets stuck in the local minima of the Q function\n",
    "- Analyse DQN instead? Skip parametrized policy function and use an approximator?? Maybe implement QT-opt https://arxiv.org/pdf/1806.10293.pdf. This may be important to obtain interesting experiment results, since on MCC the policy generally fails to follow the critic even on large learning rates (why??)\n",
    "\n",
    "11/02/2024\n",
    "- Ensemble bootstrapping (Thompson sampling) seems to have uncertain impact over baseline, maybe slightly positive?\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Confidence Bounds (How? Without latent density estimator?)\n",
    "- Termination as an action\n",
    "- Epsilon greedy\n",
    "- Time aware exploration\n",
    "\n",
    "Known Failure Modes\n",
    "- Teleporting to the end of the episode, and immediately truncating\n",
    "- \n",
    "\n",
    "Ideas\n",
    "- Bootstrapped Q value estimate for confidence bound guided estimation?\n",
    "\n",
    "Interesting observations\n",
    "- Qt_opt directly on critic, rather than target network explores faster??\n",
    "\n",
    "Reward normalisation messes up learning to reset\n",
    "\n",
    "28/02/2024\n",
    "\n",
    "Learning the reset distribution as a proper Markov chain helps a ton with lowering the requirement of resets\n",
    "It is uncertain whether the step or sigmoid reset action performs better - experiments needed. For check frequency every step, step clearly works better (sigmoid probability adds up), but sigmoid might be more fine tuned. Impact on setting on death is a lot less clear, may need better experiment. For now, recommend adding, as on certain seeds with large number of resets seems to benefit (again, experiment perhaps experiment needed). For now, use sigmoid with a check around every $10$.\n",
    "\n",
    "Teleportation impacts the reset distribution, creating a different target.\n",
    "\n",
    "2/4/2024\n",
    "Check frequency is replaced with penalty.\n",
    "This is annoying to get to work without teleportation, but works fine with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change with your own\n",
    "\n",
    "# Define Imports and shared training information\n",
    "\n",
    "# Std\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Training\n",
    "import numpy as np\n",
    "import torch\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# Evaluation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# kittenfrom kitten.common.util import *\n",
    "from cats.evaluation import *\n",
    "from cats.run import run\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"cats/config\"):\n",
    "    cfg = compose(\n",
    "        config_name=\"defaults.yaml\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "experiment_cfg = copy.deepcopy(cfg)\n",
    "# experiment_cfg.noise.scale = [0.1, 0.01]\n",
    "experiment = CatsExperiment(\n",
    "    cfg=cfg,\n",
    "    device=DEVICE\n",
    ")\n",
    "experiment.run()\n",
    "\n",
    "N_ROWS, N_COL = 1, 4\n",
    "fig, axs = plt.subplots(N_ROWS, N_COL)\n",
    "fig.set_size_inches(N_COL * 6, N_ROWS * 4)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "visualise_memory(experiment, fig, axs[0])\n",
    "visualise_experiment_value_estimate(experiment, fig, axs[1], axs[2])\n",
    "#visualise_teleport_targets(experiment, fig, axs[3])\n",
    "print(\"Entropy \", entropy_memory(experiment.memory.rb))\n",
    "print(\"Intrinsic Normalisation\", experiment.intrinsic._reward_normalisation.__str__())\n",
    "print(\"Intrinsic\", evaluate_rnd(experiment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
